{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 10270531,
          "sourceType": "datasetVersion",
          "datasetId": 6354522
        }
      ],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "#import kagglehub\n",
        "#kagglehub.login()"
      ],
      "metadata": {
        "id": "9djmnosaXQ3C"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets"
      ],
      "metadata": {
        "id": "bjAM6g2PYdaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61844ed9-e561-4a07-ccb7-dbd3f565a40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.10)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\"https://www.kaggle.com/datasets/sayan3270/mvsa-single\")"
      ],
      "metadata": {
        "id": "jv6QNBhoXQ3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cfff7d5-ea4b-4ec4-cec8-4fc612ded7b4"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./mvsa-single\" (use force=True to force download)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T06:16:31.766791Z",
          "iopub.execute_input": "2024-12-23T06:16:31.767125Z",
          "iopub.status.idle": "2024-12-23T06:16:40.996502Z",
          "shell.execute_reply.started": "2024-12-23T06:16:31.767096Z",
          "shell.execute_reply": "2024-12-23T06:16:40.995416Z"
        },
        "id": "MD4qrLYuXQ3F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install opendatasets transformers torch torchvision matplotlib seaborn"
      ],
      "metadata": {
        "trusted": true,
        "id": "s1X549NzXQ3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96721236-08b9-4f32-dc77-c06b49afecf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.12.14)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "from torch.cuda.amp import GradScaler, autocast\n"
      ],
      "metadata": {
        "id": "Nj33fVCNfklU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "ypCEQwijfo_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading functions\n",
        "def load_text_data(data_folder):\n",
        "    texts = []\n",
        "    filenames = sorted(os.listdir(data_folder), key=lambda x: int(x[:-4]) if x[:-4].isdigit() else x)\n",
        "    for filename in filenames:\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(data_folder, filename), 'r', encoding='latin-1') as file:\n",
        "                text = file.read().strip()\n",
        "                texts.append(text)\n",
        "    return texts, filenames\n",
        "\n",
        "def load_labels(result_file):\n",
        "    labels = {}\n",
        "    with open(result_file, 'r') as file:\n",
        "        next(file)  # Skip header\n",
        "        for line in file:\n",
        "            parts = line.strip().split('\\t')\n",
        "            text_id = int(parts[0])\n",
        "            text_label, image_label = parts[1].split(',')\n",
        "            labels[text_id] = text_label.strip()\n",
        "    return labels\n",
        "\n",
        "def filter_existing_files(texts, filenames, labels, data_folder):\n",
        "    existing_texts = []\n",
        "    existing_images = []\n",
        "    existing_labels = []\n",
        "    for i, text in enumerate(texts):\n",
        "        image_file = os.path.join(data_folder, f\"{i+1}.jpg\")\n",
        "        if os.path.exists(image_file) and (i+1) in labels:\n",
        "            existing_texts.append(text)\n",
        "            existing_images.append(image_file)\n",
        "            existing_labels.append(labels[i+1])\n",
        "    return existing_texts, existing_images, existing_labels\n"
      ],
      "metadata": {
        "id": "B0UCbZfrfrcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and save data\n",
        "def preprocess_and_save(texts, image_paths, labels, tokenizer, transform, save_path):\n",
        "    processed_data = []\n",
        "    sentiment_to_label = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "    for text, image_path, label in zip(texts, image_paths, labels):\n",
        "        encoded_text = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "        image = transform(Image.open(image_path).convert('RGB'))\n",
        "        processed_data.append((encoded_text, image, sentiment_to_label[label]))\n",
        "    with open(save_path, 'wb') as f:\n",
        "        pickle.dump(processed_data, f)"
      ],
      "metadata": {
        "id": "NGhxy_cPfwKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load processed data\n",
        "def load_processed_data(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n"
      ],
      "metadata": {
        "id": "iEOacWtsfzCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, processed_data):\n",
        "        self.processed_data = processed_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded_text, image, label = self.processed_data[idx]\n",
        "        return {\n",
        "            'text': encoded_text['input_ids'].squeeze(),\n",
        "            'attention_mask': encoded_text['attention_mask'].squeeze(),\n",
        "            'image': image,\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "aWQUL9a3f2F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "data_folder = \"mvsa-single/MVSA_Single/data\"\n",
        "result_file = 'mvsa-single/MVSA_Single/labelResultAll.txt'\n",
        "train_data_path = 'train_data.pkl'\n",
        "val_data_path = 'val_data.pkl'"
      ],
      "metadata": {
        "id": "U3C2tiDPf4zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "texts, filenames = load_text_data(data_folder)\n",
        "labels = load_labels(result_file)\n",
        "texts, image_paths, labels = filter_existing_files(texts, filenames, labels, data_folder)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_texts, val_texts, train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "    texts, image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "preprocess_and_save(train_texts, train_images, train_labels, tokenizer, train_transform, train_data_path)\n",
        "preprocess_and_save(val_texts, val_images, val_labels, tokenizer, val_transform, val_data_path)\n",
        "\n",
        "train_data = load_processed_data(train_data_path)\n",
        "val_data = load_processed_data(val_data_path)\n",
        "\n",
        "train_dataset = MultimodalDataset(train_data)\n",
        "val_dataset = MultimodalDataset(val_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
      ],
      "metadata": {
        "id": "z7z9wTzaf7GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model class\n",
        "class MultimodalSentimentModel(nn.Module):\n",
        "    def __init__(self, bert_model, resnet_model, num_classes):\n",
        "        super(MultimodalSentimentModel, self).__init__()\n",
        "        self.text_model = bert_model\n",
        "        self.image_model = resnet_model\n",
        "\n",
        "        self.text_output_size = 768\n",
        "        self.image_output_size = 25088  # Changed to 25088 to match VGG19 output\n",
        "\n",
        "        # The input size to the linear layer should match the output size from previous layer.\n",
        "        # The output of concatenation in the forward() method will be (batch_size, self.text_output_size + self.image_output_size)\n",
        "        self.fc1 = nn.Linear(self.text_output_size + self.image_output_size, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, image):\n",
        "        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
        "        image_output = self.image_model(image)\n",
        "        # image_output = image_output.view(image_output.size(0), -1) # flatten vgg19 output\n",
        "        # image_output = image_output.reshape(image_output.shape[0], -1)\n",
        "\n",
        "        combined = torch.cat((text_output, image_output), dim=1)\n",
        "        x = self.fc1(combined)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kA3EHdM8f-99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torchvision.models import vgg16, vgg19\n",
        "\n",
        "# Initialize models\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "image_model = vgg19(pretrained=True)\n",
        "image_model.classifier = nn.Identity()  # Remove final classification layer\n",
        "\n",
        "model = MultimodalSentimentModel(bert_model, image_model, num_classes=3).to(device)\n"
      ],
      "metadata": {
        "id": "s-9x0N0_gE-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69d3492-b284-40e2-f112-63d0cc3b7804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
        "scaler = GradScaler()"
      ],
      "metadata": {
        "id": "DWDIxyiDgMdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeaa9d93-1755-4eb6-f9f4-98d5c64321dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-1603c3bf1759>:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "patience = 10\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['text'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        images = batch['image'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast('cuda'):  # Updated usage\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        train_total += labels.size(0)\n",
        "        train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc = train_correct / train_total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['text'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "nAesTksIgdQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46257420-b904-400d-b7e5-7fb8c6ad0f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50: Train Loss: 1.0882, Train Acc: 0.4041\n",
            "Val Loss: 1.0773, Val Acc: 0.3991\n",
            "Epoch 2/50: Train Loss: 0.9887, Train Acc: 0.5018\n",
            "Val Loss: 1.1236, Val Acc: 0.4111\n",
            "Epoch 3/50: Train Loss: 0.7137, Train Acc: 0.6922\n",
            "Val Loss: 1.2440, Val Acc: 0.4056\n",
            "Epoch 4/50: Train Loss: 0.2943, Train Acc: 0.8888\n",
            "Val Loss: 2.1474, Val Acc: 0.3970\n",
            "Epoch 5/50: Train Loss: 0.1222, Train Acc: 0.9601\n",
            "Val Loss: 3.0492, Val Acc: 0.4002\n",
            "Epoch 6/50: Train Loss: 0.0389, Train Acc: 0.9878\n",
            "Val Loss: 3.1335, Val Acc: 0.4284\n",
            "Epoch 7/50: Train Loss: 0.0140, Train Acc: 0.9984\n",
            "Val Loss: 3.2981, Val Acc: 0.4295\n",
            "Epoch 8/50: Train Loss: 0.0076, Train Acc: 0.9989\n",
            "Val Loss: 3.5769, Val Acc: 0.4208\n",
            "Epoch 9/50: Train Loss: 0.0049, Train Acc: 0.9997\n",
            "Val Loss: 3.8213, Val Acc: 0.4165\n",
            "Epoch 10/50: Train Loss: 0.0036, Train Acc: 0.9997\n",
            "Val Loss: 3.8400, Val Acc: 0.4187\n",
            "Epoch 11/50: Train Loss: 0.0032, Train Acc: 0.9997\n",
            "Val Loss: 3.8617, Val Acc: 0.4208\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save the model\n",
        "\n",
        "# Assuming 'model' is your trained model\n",
        "torch.save(model.state_dict(), 'best_model.pth')"
      ],
      "metadata": {
        "id": "6xFcIs5viZCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save the model\n",
        "\n",
        "# Assuming 'model' is your trained model\n",
        "torch.save(model.state_dict(), 'best_model.keras')"
      ],
      "metadata": {
        "id": "UAQ99Jtjij1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate sensitivity and specificity\n",
        "def calculate_sensitivity_specificity(conf_matrix):\n",
        "    # Assuming binary classification, class 0 is negative and class 1 is positive\n",
        "    TP = conf_matrix[1, 1]\n",
        "    TN = conf_matrix[0, 0]\n",
        "    FP = conf_matrix[0, 1]\n",
        "    FN = conf_matrix[1, 0]\n",
        "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "    return sensitivity, specificity\n",
        "\n",
        "model.eval()\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch['text'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        images = batch['image'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask, images)\n",
        "        probabilities = torch.softmax(outputs, dim=1)  # Convert logits to probabilities\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_labels = np.array(all_labels)\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_probabilities = np.array(all_probabilities)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "sensitivity, specificity = calculate_sensitivity_specificity(conf_matrix)\n",
        "roc_auc = roc_auc_score(all_labels, all_probabilities, multi_class='ovr')  # Assuming multi-class classification\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMzRFzXFtSUH",
        "outputId": "3ac2b93e-60dd-46a9-a2f1-8cd799f369cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4208\n",
            "F1 Score: 0.4168\n",
            "Sensitivity: 0.6784\n",
            "Specificity: 0.4194\n",
            "AUC-ROC: 0.5656\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.28      0.31       232\n",
            "           1       0.43      0.43      0.43       358\n",
            "           2       0.45      0.51      0.48       332\n",
            "\n",
            "    accuracy                           0.42       922\n",
            "   macro avg       0.41      0.41      0.41       922\n",
            "weighted avg       0.42      0.42      0.42       922\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "\n",
        "# Example inputs\n",
        "example_text = \"How I feel today #legday #jelly #aching #gym \"\n",
        "example_image_path = \"/content/mvsa-single/MVSA_Single/data/1.jpg\"  # Replace with the actual image path\n",
        "\n",
        "# Load the tokenizer and transformation (use the same as in preprocessing)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Preprocess the inputs\n",
        "encoded_text = tokenizer(example_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "image = transform(Image.open(example_image_path).convert('RGB'))\n",
        "\n",
        "# Move inputs to the appropriate device\n",
        "input_ids = encoded_text['input_ids'].to(device)\n",
        "attention_mask = encoded_text['attention_mask'].to(device)\n",
        "image = image.unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "# Pass through the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask, image)\n",
        "    probabilities = torch.softmax(outputs, dim=1)\n",
        "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "# Map the prediction to sentiment\n",
        "label_to_sentiment = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}  # Adjust based on your label mapping\n",
        "predicted_sentiment = label_to_sentiment[predicted_class]\n",
        "\n",
        "# Print the results\n",
        "print(f\"Predicted Sentiment: {predicted_sentiment}\")\n",
        "print(f\"Probabilities: {probabilities.cpu().numpy()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sotY70PvutFI",
        "outputId": "02f382b6-fbd7-40cc-9d64-cdc0fb8bd921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Sentiment: Neutral\n",
            "Probabilities: [[4.8393791e-04 9.9951279e-01 3.2185587e-06]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the model\n",
        "with open('multimodal_sentiment_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "print(\"Model saved to multimodal_sentiment_model.pkl\")\n"
      ],
      "metadata": {
        "id": "X5oXWVqMgngA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbcf4ae0-3b2c-4226-dd81-590056d31c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to multimodal_sentiment_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***METHODOLOGY***\n",
        "---\n",
        "\n",
        "1. Dataset Preparation:\n",
        "\n",
        "*   Text Data Loading: Load textual data from the dataset, process it using a tokenizer (BERT tokenizer in this case), and store the results.\n",
        "*   Image Data Loading: Load images from the dataset and preprocess them using transformations (resize, crop, normalization).\n",
        "\n",
        "\n",
        "*  Label Loading: Read labels from the dataset and map them to sentiment classes (e.g., negative, neutral, positive).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. Data Preprocessing:\n",
        "\n",
        "* Filter out incomplete samples where both text and image data are not available.\n",
        "\n",
        "* Tokenize text using BERT tokenizer and apply transformations to images.\n",
        "\n",
        "* Encode the sentiment labels into numerical values.\n",
        "\n",
        "---\n",
        "\n",
        "3. Data Splitting:\n",
        "\n",
        "* Split the dataset into training and validation sets using a stratified approach to maintain class balance.\n",
        "\n",
        "---\n",
        "\n",
        "4. Data Serialization:\n",
        "\n",
        "Save the preprocessed data (text, images, and labels) into .pkl files for faster loading during training.\n",
        "\n",
        "---\n",
        "\n",
        "5. Model Design:\n",
        "\n",
        "* Textual Model: Use the BERT model to encode the text input into a feature vector.\n",
        "\n",
        "* Visual Model: Use a ResNet50 model pre-trained on ImageNet to extract image features.\n",
        "\n",
        "* Fusion Layer: Concatenate text and image features and pass them through fully connected layers to predict sentiment.\n",
        "\n",
        "---\n",
        "\n",
        "6. Training:\n",
        "\n",
        "* Use CrossEntropyLoss as the loss function.\n",
        "\n",
        "* Use Adam optimizer with learning rate scheduling.\n",
        "\n",
        "* Utilize mixed precision training for performance improvement.\n",
        "\n",
        "* Implement early stopping based on validation loss to prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "7. Evaluation:\n",
        "\n",
        "* Compute evaluation metrics: Accuracy, F1-score, Sensitivity, Specificity, and AUC-ROC.\n",
        "\n",
        "* Generate a confusion matrix and classification report for detailed performance analysis.\n",
        "\n",
        "---\n",
        "\n",
        "8. Inference:\n",
        "\n",
        "* Preprocess input text and image.\n",
        "\n",
        "* Pass the inputs through the trained model to predict sentiment and output probabilities.\n",
        "\n",
        "---\n",
        "\n",
        "9. Model Serialization:\n",
        "\n",
        "* Save the trained model into a .pkl file for future use.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ieeTJWolvgxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***FLOWCHART***\n",
        "---\n",
        "\n",
        "1. Text Representation\n",
        "\n",
        "* Text Input\n",
        "\n",
        "Raw text → BERT Tokenizer → Tokenized Text → Embedding Representation\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "2. Image Representation\n",
        "\n",
        "Raw Image → Preprocessing (Resizing, Cropping, Normalization) → ResNet50 → Feature Vector\n",
        "\n",
        "---\n",
        "\n",
        "3. Sentiment Prediction\n",
        "\n",
        "* Feature Fusion\n",
        "\n",
        "Text Features + Image Features → Concatenation → Fully Connected Layers → Sentiment Prediction\n",
        "\n",
        "---\n",
        "\n",
        "4. Output\n",
        "\n",
        "* Sentiment Class: Negative, Neutral, Positive\n",
        "\n",
        "* Metrics: Accuracy, F1-Score, Sensitivity, Specificity, AUC-ROC\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yERV0tRfxNJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Example Workflow***\n",
        "---\n",
        "\n",
        "1. Input Preparation\n",
        "\n",
        "* Example Text: \"The product is amazing!\"\n",
        "\n",
        "* Example Image: Path to the product image.\n",
        "\n",
        "---\n",
        "\n",
        "2. Preprocessing\n",
        "\n",
        "* Tokenize text and transform the image.\n",
        "\n",
        "---\n",
        "\n",
        "3. Model Inference\n",
        "\n",
        "* Pass tokenized text and image through the model.\n",
        "\n",
        "---\n",
        "\n",
        "4. Prediction\n",
        "\n",
        "* Output: Sentiment class (e.g., Positive) and probabilities for each class.\n",
        "\n",
        "---\n",
        "\n",
        "5. Evaluation\n",
        "\n",
        "* Calculate metrics using validation data and visualize performance through a confusion matrix and ROC curves.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VnNvhdyCx3up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 1 (50 epochs)\n",
        "Accuracy: 0.5304\n",
        "F1 Score: 0.5291\n",
        "Sensitivity: 0.6211\n",
        "Specificity: 0.6425\n",
        "AUC-ROC: 0.7102\n",
        "Option 2 (10 epochs)\n",
        "Accuracy: 0.5390\n",
        "F1 Score: 0.5385\n",
        "Sensitivity: 0.7283\n",
        "Specificity: 0.6114\n",
        "AUC-ROC: 0.7057\n",
        "Key Considerations:\n",
        "Accuracy:\n",
        "\n",
        "Option 2 has a slightly higher accuracy (0.5390 vs. 0.5304). While accuracy can be informative, it may not always reflect performance in imbalanced datasets, where sensitivity and specificity matter more.\n",
        "F1 Score:\n",
        "\n",
        "Option 2 has a higher F1 Score (0.5385 vs. 0.5291). The F1 Score considers both precision and recall, so it provides a balanced view of model performance, especially in the case of class imbalance.\n",
        "Sensitivity (Recall for class 1):\n",
        "\n",
        "Option 2 has better sensitivity (0.7283 vs. 0.6211), which is important for identifying the positive class (class 1 in this case). This means Option 2 performs better in correctly identifying positive instances.\n",
        "Specificity (True Negative Rate for class 0):\n",
        "\n",
        "Option 1 has a higher specificity (0.6425 vs. 0.6114). While specificity is important, it is less critical in many cases compared to sensitivity, especially when false negatives (missed positive cases) are more costly than false positives.\n",
        "AUC-ROC:\n",
        "\n",
        "Option 1 has a slightly higher AUC-ROC (0.7102 vs. 0.7057). A higher AUC-ROC indicates a better ability to distinguish between classes, but the difference is marginal.\n",
        "Conclusion:\n",
        "While Option 1 has a better AUC-ROC and specificity, Option 2 has better sensitivity, F1 Score, and accuracy. Given that sensitivity (recall for the positive class) and the F1 Score are generally more important in a classification problem with imbalanced classes (which is likely here), Option 2 is better.\n",
        "\n",
        "Why Option 2 is better:\n",
        "It has a higher sensitivity, which means it is better at identifying positive cases, which is often more crucial in real-world applications where missed positive instances can have more significant consequences.\n",
        "The F1 Score is higher, indicating better overall performance across precision and recall.\n",
        "The performance is achieved in fewer epochs (10 epochs vs. 50 epochs), which suggests the model is more efficient."
      ],
      "metadata": {
        "id": "HWc4KFMyAh7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "while True:\n",
        "    print(\"Preventing timeout...\")\n",
        "    time.sleep(300)  # Sleeps for 5 minutes (300 seconds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "QJTctGZxi-ty",
        "outputId": "4f59b09f-a593-4d6c-be0f-5ac71ec16b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preventing timeout...\n",
            "Preventing timeout...\n",
            "Preventing timeout...\n",
            "Preventing timeout...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-2400011ad48f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preventing timeout...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sleeps for 5 minutes (300 seconds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/multimodal_sentiment_model.pkl')\n"
      ],
      "metadata": {
        "id": "TKFrElzqiMLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/model.keras')"
      ],
      "metadata": {
        "id": "7ueP2wQc_cQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/best_model.pth')"
      ],
      "metadata": {
        "id": "zrEwzbVN_lPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rn1ZuQ3G_qOH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}